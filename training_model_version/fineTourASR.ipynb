{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/audio/Recording_1.wav</td>\n",
       "      <td>ยานี้ชื่อไอบูโปรเพนความแรง400มิลลิกรัมจำนวน10เม็ด</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/audio/Recording_2.wav</td>\n",
       "      <td>ยานี้ชื่ออะมอกซีซิลินความแรง500มิลลิกรัมจำนวน3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/audio/Recording_3.wav</td>\n",
       "      <td>ยานี้ชื่อเด็กซ์ออฟจำนวน1ขวดใช้สำหรับหยอดหู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/audio/Recording_4.wav</td>\n",
       "      <td>สวัสดีครับผมเป็นเภสัชกร</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/audio/Recording_5.wav</td>\n",
       "      <td>หนูเป็นเภสัชกรค่ะ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/audio/Recording_6.wav</td>\n",
       "      <td>มีอาการปวดหูที่ข้างซ้ายใช่ไหมคะ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/audio/Recording_7.wav</td>\n",
       "      <td>คนไข้ตั้งครรภ์ไหมคะ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/audio/Recording_8.wav</td>\n",
       "      <td>ความดันคนไข้สูงนะคะ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/audio/Recording_9.wav</td>\n",
       "      <td>ยานี้ควรรับประทานติดต่อกันทุกวันจนหมด</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/audio/Recording_10.wav</td>\n",
       "      <td>ล้างมือให้สะอาด</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/audio/Recording_11.wav</td>\n",
       "      <td>นอนตะแคงหรือนั่งเอียงศีรษะให้หูข้างที่จะหยอดยา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/audio/Recording_12.wav</td>\n",
       "      <td>ให้ดึงใบหูข้างที่จะหยอดยาไปด้านหลังและดึงขึ้นด...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/audio/Recording_13.wav</td>\n",
       "      <td>หยอดยาเข้าไปในหูตามที่แพทย์สั่ง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/audio/Recording_14.wav</td>\n",
       "      <td>ระวังอย่าเอาหลอดหยดสอดเข้าไปในรูหู</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/audio/Recording_15.wav</td>\n",
       "      <td>เอียงตะแคงอยู่ท่าเดิมประมาณ3ถึง5นาที</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/audio/Recording_16.wav</td>\n",
       "      <td>อาจเอาสำลีใส่ในรูหูไว้อีกระยะหนึ่งเพื่อไม่ให้ย...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/audio/Recording_17.wav</td>\n",
       "      <td>ให้เขย่าขวดก่อนใช้ทุกครั้ง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/audio/Recording_18.wav</td>\n",
       "      <td>ใช้ไม้พันสำลีเช็ดหนองในใบหูหรือรูหูให้สะอาดก่อ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/audio/Recording_19.wav</td>\n",
       "      <td>ยาหยอดหูหากเก็บไว้ในตู้เย็นก่อนใช้ยาให้กำหลอดย...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data/audio/Recording_20.wav</td>\n",
       "      <td>เพื่อปรับอุณหภูมิให้ใกล้เคียงกับร่างกายก่อนหยอด</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/audio/Recording_21.wav</td>\n",
       "      <td>ใช้รักษาหูชั้นกลางอักเสบ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/audio/Recording_22.wav</td>\n",
       "      <td>โดยหยอดหูซ้าย3หยดวันละ3ครั้งเช้ากลางวันเย็น</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>data/audio/Recording_23.wav</td>\n",
       "      <td>วันนี้มารับยา3ตัวนะครับยานี้ชื่อไอบูโปรเพนความ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>data/audio/Recording_24.wav</td>\n",
       "      <td>ยานี้ควรรับประทานหลังอาหารทันที</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           path  \\\n",
       "0    data/audio/Recording_1.wav   \n",
       "1    data/audio/Recording_2.wav   \n",
       "2    data/audio/Recording_3.wav   \n",
       "3    data/audio/Recording_4.wav   \n",
       "4    data/audio/Recording_5.wav   \n",
       "5    data/audio/Recording_6.wav   \n",
       "6    data/audio/Recording_7.wav   \n",
       "7    data/audio/Recording_8.wav   \n",
       "8    data/audio/Recording_9.wav   \n",
       "9   data/audio/Recording_10.wav   \n",
       "10  data/audio/Recording_11.wav   \n",
       "11  data/audio/Recording_12.wav   \n",
       "12  data/audio/Recording_13.wav   \n",
       "13  data/audio/Recording_14.wav   \n",
       "14  data/audio/Recording_15.wav   \n",
       "15  data/audio/Recording_16.wav   \n",
       "16  data/audio/Recording_17.wav   \n",
       "17  data/audio/Recording_18.wav   \n",
       "18  data/audio/Recording_19.wav   \n",
       "19  data/audio/Recording_20.wav   \n",
       "20  data/audio/Recording_21.wav   \n",
       "21  data/audio/Recording_22.wav   \n",
       "22  data/audio/Recording_23.wav   \n",
       "23  data/audio/Recording_24.wav   \n",
       "\n",
       "                                                 text  \n",
       "0   ยานี้ชื่อไอบูโปรเพนความแรง400มิลลิกรัมจำนวน10เม็ด  \n",
       "1   ยานี้ชื่ออะมอกซีซิลินความแรง500มิลลิกรัมจำนวน3...  \n",
       "2          ยานี้ชื่อเด็กซ์ออฟจำนวน1ขวดใช้สำหรับหยอดหู  \n",
       "3                             สวัสดีครับผมเป็นเภสัชกร  \n",
       "4                                   หนูเป็นเภสัชกรค่ะ  \n",
       "5                     มีอาการปวดหูที่ข้างซ้ายใช่ไหมคะ  \n",
       "6                                 คนไข้ตั้งครรภ์ไหมคะ  \n",
       "7                                 ความดันคนไข้สูงนะคะ  \n",
       "8               ยานี้ควรรับประทานติดต่อกันทุกวันจนหมด  \n",
       "9                                    ล้างมือให้สะอาด   \n",
       "10  นอนตะแคงหรือนั่งเอียงศีรษะให้หูข้างที่จะหยอดยา...  \n",
       "11  ให้ดึงใบหูข้างที่จะหยอดยาไปด้านหลังและดึงขึ้นด...  \n",
       "12                   หยอดยาเข้าไปในหูตามที่แพทย์สั่ง   \n",
       "13                ระวังอย่าเอาหลอดหยดสอดเข้าไปในรูหู   \n",
       "14              เอียงตะแคงอยู่ท่าเดิมประมาณ3ถึง5นาที   \n",
       "15  อาจเอาสำลีใส่ในรูหูไว้อีกระยะหนึ่งเพื่อไม่ให้ย...  \n",
       "16                        ให้เขย่าขวดก่อนใช้ทุกครั้ง   \n",
       "17  ใช้ไม้พันสำลีเช็ดหนองในใบหูหรือรูหูให้สะอาดก่อ...  \n",
       "18  ยาหยอดหูหากเก็บไว้ในตู้เย็นก่อนใช้ยาให้กำหลอดย...  \n",
       "19   เพื่อปรับอุณหภูมิให้ใกล้เคียงกับร่างกายก่อนหยอด   \n",
       "20                           ใช้รักษาหูชั้นกลางอักเสบ  \n",
       "21       โดยหยอดหูซ้าย3หยดวันละ3ครั้งเช้ากลางวันเย็น   \n",
       "22  วันนี้มารับยา3ตัวนะครับยานี้ชื่อไอบูโปรเพนความ...  \n",
       "23                    ยานี้ควรรับประทานหลังอาหารทันที  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"data/ASRdataset.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'text'],\n",
       "    num_rows: 24\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'text'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['path', 'text'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"scb10x/monsoon-whisper-medium-gigaspeech2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"scb10x/monsoon-whisper-medium-gigaspeech2\", language=\"th\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"scb10x/monsoon-whisper-medium-gigaspeech2\", language=\"th\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array(file_path):\n",
    "    audio_input, samplerate = librosa.load(file_path, sr=16000)\n",
    "    return audio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    print(batch['path'])\n",
    "    arr = get_array(f'{batch['path']}')\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(arr, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch['text']).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8047a6d6e64a4bad8e401e37696f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/audio/Recording_1.wav\n",
      "data/audio/Recording_10.wav\n",
      "data/audio/Recording_18.wav\n",
      "data/audio/Recording_24.wav\n",
      "data/audio/Recording_13.wav\n",
      "data/audio/Recording_7.wav\n",
      "data/audio/Recording_3.wav\n",
      "data/audio/Recording_8.wav\n",
      "data/audio/Recording_11.wav\n",
      "data/audio/Recording_2.wav\n",
      "data/audio/Recording_16.wav\n",
      "data/audio/Recording_4.wav\n",
      "data/audio/Recording_14.wav\n",
      "data/audio/Recording_23.wav\n",
      "data/audio/Recording_6.wav\n",
      "data/audio/Recording_21.wav\n",
      "data/audio/Recording_22.wav\n",
      "data/audio/Recording_5.wav\n",
      "data/audio/Recording_15.wav\n",
      "data/audio/Recording_19.wav\n",
      "data/audio/Recording_20.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cca804637ac49b0943a4c386f88c0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/audio/Recording_17.wav\n",
      "data/audio/Recording_9.wav\n",
      "data/audio/Recording_12.wav\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'text', 'input_features', 'labels'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['path', 'text', 'input_features', 'labels'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"scb10x/monsoon-whisper-medium-gigaspeech2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"th\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-monsoon-t1\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=3.0,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    optim=\"adamw_torch_fused\", # adamw_torch_fused, adamw_8bit, adamw_torch, sgd\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=256,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=5,\n",
    "    logging_steps=50,\n",
    "    report_to=None,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-monsoon-t1\",  # change to a repo name of your choice\n",
    "#     per_device_train_batch_size=16,\n",
    "#     gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "#     learning_rate=1e-5,\n",
    "#     warmup_steps=500,\n",
    "#     max_steps=4000,\n",
    "#     gradient_checkpointing=True,\n",
    "#     fp16=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     predict_with_generate=True,\n",
    "#     generation_max_length=225,\n",
    "#     save_steps=1000,\n",
    "#     eval_steps=1000,\n",
    "#     logging_steps=25,\n",
    "#     report_to=[\"tensorboard\"],\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"wer\",\n",
    "#     greater_is_better=False,\n",
    "#     push_to_hub=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_19956\\2774941355.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model.to(device),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 24:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.162984</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.161941</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.118921</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50289], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "c:\\Users\\LENOVO\\Desktop\\DemoP\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=0.841770225101047, metrics={'train_runtime': 1499.1443, 'train_samples_per_second': 0.042, 'train_steps_per_second': 0.012, 'total_flos': 6.429810917376e+16, 'train_loss': 0.841770225101047, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./whisper-monsoon-t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './whisper-monsoon-t1/trainer_state.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load training logs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m log_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./whisper-monsoon-t1/trainer_state.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     logs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Extract loss values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Desktop\\DemoP\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './whisper-monsoon-t1/trainer_state.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training logs\n",
    "log_file = \"./whisper-monsoon-t1/trainer_state.json\"\n",
    "with open(log_file, \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Extract loss values\n",
    "steps = [x[\"step\"] for x in logs[\"log_history\"] if \"loss\" in x]\n",
    "losses = [x[\"loss\"] for x in logs[\"log_history\"] if \"loss\" in x]\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(steps, losses, marker=\"o\", linestyle=\"-\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 0EC5-F6D5\n",
      "\n",
      " Directory of c:\\Users\\LENOVO\\Desktop\\DemoP\\flask\n",
      "\n",
      "02/19/2025  09:43 AM    <DIR>          .\n",
      "02/07/2025  08:43 PM    <DIR>          ..\n",
      "02/07/2025  08:38 PM               768 ASR.py\n",
      "02/12/2025  09:10 PM    <DIR>          asr_finetuned\n",
      "02/07/2025  08:28 PM    <DIR>          audio\n",
      "02/18/2025  05:40 PM    <DIR>          checkpoints\n",
      "02/07/2025  08:31 PM         1,564,844 converted.wav\n",
      "02/18/2025  08:23 PM    <DIR>          data\n",
      "02/18/2025  08:05 PM            20,784 evalanotherdata.ipynb\n",
      "02/13/2025  10:09 AM         1,150,346 findTuningASR.ipynb\n",
      "02/19/2025  12:53 PM            30,423 findTuningSentT.ipynb\n",
      "02/21/2025  07:37 AM            32,815 fineTourASR.ipynb\n",
      "02/18/2025  06:35 PM         1,423,433 flow.ipynb\n",
      "02/05/2025  02:41 PM    <DIR>          mms\n",
      "02/18/2025  06:34 PM            38,567 recording_results.xlsx\n",
      "02/07/2025  08:52 PM             7,133 server.py\n",
      "02/06/2025  08:33 PM            69,690 techno_output.wav\n",
      "02/09/2025  12:58 PM    <DIR>          trained_model\n",
      "02/13/2025  08:41 AM            88,009 valid.ipynb\n",
      "02/21/2025  07:27 AM    <DIR>          whisper-monsoon-t1\n",
      "02/13/2025  10:06 AM    <DIR>          whisper-small-hi\n",
      "              11 File(s)      4,426,812 bytes\n",
      "              10 Dir(s)   6,021,652,480 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "\n",
    "# Define model path\n",
    "model_path = \"./whisper-monsoon-t1\"\n",
    "\n",
    "# Load processor (tokenizer + feature extractor)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio, sr, max_length_sec=30):\n",
    "    max_samples = sr * max_length_sec\n",
    "    return [audio[i:i + max_samples] for i in range(0, len(audio), max_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASR(file_path):\n",
    "    sr=16000\n",
    "    # Load audio file\n",
    "    audio_input, _ = librosa.load(file_path, sr=sr)  # Ensure 16kHz sample rate\n",
    "\n",
    "    # Split audio into chunks\n",
    "    audio_chunks = split_audio(audio_input, sr)\n",
    "    \n",
    "    transcriptions = []\n",
    "    \n",
    "    for chunk in audio_chunks:\n",
    "        # Process the chunk to match model input requirements, \n",
    "        # include any generation parameters as needed (e.g., language)\n",
    "        input_features = processor(chunk, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "        \n",
    "        # Generate token IDs using the model\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        \n",
    "        # Decode token IDs to text, skipping any special tokens\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        transcriptions.append(transcription)\n",
    "    \n",
    "    # Combine transcriptions from all chunks (add a separator if needed)\n",
    "    return \"\".join(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_12392\\3335953257.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_input, _ = librosa.load(file_path, sr=sr)  # Ensure 16kHz sample rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "วันนี้มารับยา3ตัวนะครับยานี้ชื่อไอบูโปรเพนความแรง400มิลลิกรัมจำนวนเม็ดใช้เพื่อบรรเทาอาการปวดรับประทานครั้งละยานี้ควรรับประทานติดต่อกันทุกวันจนหมดและยานี้ชื่อเด็กซ์ออฟจำนวน1ขวดใช้สำหรับหยอดหูเพื่อรักษาอาการติดเชื้อที่หูหยอดสามหยอดที่หูข้างซ้ายวันละสามครั้งหลังอาหารเช้ากลางวันเย็น \n"
     ]
    }
   ],
   "source": [
    "print(ASR(r'data\\test\\recording.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
